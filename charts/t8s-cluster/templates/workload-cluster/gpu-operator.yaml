{{- if eq (include "t8s-cluster.hasGPUNodes" (dict "context" $)) "true" }}
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: {{ printf "%s-gpu-operator" .Release.Name }}
  namespace: {{ .Release.Namespace}}
  labels: {{- include "common.labels.standard" . | nindent 4 }}
spec:
  chart:
    spec: {{- include "common.helm.chartSpec" (dict "repo" "nvidia" "prependReleaseName" true "chart" "gpu-operator" "context" $) | nindent 6 }}
  interval: 1h
  driftDetection:
    mode: enabled
  kubeConfig:
    secretRef:
      name: {{ .Release.Name }}-kubeconfig
  install:
    remediation:
      retries: -1
  upgrade:
    timeout: 5m
    crds: CreateReplace
  storageNamespace: kube-system
  targetNamespace: kube-system
  releaseName: gpu-operator
  values:
    devicePlugin:
      config:
        create: true
        name: gpu-operator-time-slicing
        default: any
        data:
          any: |-
            version: v1
            sharing:
              timeSlicing:
                failRequestsGreaterThanOne: false
                resources:
                  - name: nvidia.com/gpu
                    replicas: 4
    node-feature-discovery:
      worker:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                    - key: node.kubernetes.io/instance-type
                      operator: In
                      {{- $gpuFlavours := list }}
                      {{- range $_, $machineDeploymentClass := .Values.nodePools -}}
                        {{- if contains "gpu" (lower $machineDeploymentClass.flavor) -}}
                          {{- $gpuFlavours = append $gpuFlavours $machineDeploymentClass.flavor -}}
                        {{- end -}}
                      {{- end }}
                      values: {{- toYaml $gpuFlavours | nindent 24 }}
{{- end }}
