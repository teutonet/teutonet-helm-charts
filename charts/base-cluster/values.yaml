global:
  serviceLevelAgreement: None
  clusterName: ""
  baseDomain: ""
  imageRegistry: ""
  imageCredentials: {}
  priorityClasses: {}
  namespaces:
    ingress:
      condition: '{{ or (not (empty .Values.dns.provider)) (eq .Values.ingress.provider "traefik") }}'
      additionalLabels:
        app.kubernetes.io/component: ingress
    cert-manager:
      additionalLabels:
        app.kubernetes.io/component: cert-manager
      resources:
        defaults:
          requests:
            cpu: 50m
            memory: 100Mi
          limits:
            cpu: 500m
            memory: 300Mi
        quotas:
          limits.cpu: "2"
          limits.memory: 2Gi
    ingress-nginx:
      condition: '{{ eq .Values.ingress.provider "nginx" }}'
      additionalLabels:
        app.kubernetes.io/component: ingress
    kyverno:
      condition: "{{ .Values.kyverno.enabled }}"
      additionalLabels:
        app.kubernetes.io/component: kyverno
    monitoring:
      condition: "{{ or .Values.monitoring.prometheus.enabled .Values.monitoring.metricsServer.enabled }}"
      additionalLabels:
        app.kubernetes.io/component: monitoring
      resources:
        defaults:
          requests:
            cpu: 20m
            memory: 100Mi
    trivy:
      condition: "{{ .Values.monitoring.securityScanning.enabled }}"
      additionalLabels:
        app.kubernetes.io/component: security
    nfs-server-provisioner:
      condition: "{{ .Values.storage.readWriteMany.enabled }}"
      additionalLabels:
        app.kubernetes.io/component: storage
        app.kubernetes.io/part-of: nfs-server-provisioner
    backup:
      condition: "{{ not (empty .Values.backup.provider) }}"
      additionalLabels:
        app.kubernetes.io/component: backup
  certificates:
    cluster-wildcard:
      dnsNames: |-
        - {{ printf "*.%s" (include "base-cluster.domain" $) | quote }}
      targetNamespaces: ALL
      condition: "{{ and (not (empty .Values.global.baseDomain)) (not (empty .Values.dns.provider)) }}"
  storageClass: ""
  kubectl:
    image:
      registry: registry.k8s.io
      repository: kubectl
      tag: 1.33.4@sha256:261a9ed843eb68e3d50da132245e2221d75ca19504130e47bd32788c0ff339a0
  flux:
    image:
      registry: docker.io
      repository: fluxcd/flux-cli
      tag: v2.7.5@sha256:3a4f9b0a26c26db8193c476e0abe33c32187eae8ce7a0a604d38a51783544946
  gpg:
    image:
      registry: docker.io
      repository: vladgh/gpg
      tag: 1.3.7@sha256:447ee5ff23e5fd8a48a8586dcbe266289cd83a222ff9aebed91a77306fd7ecad
  curl:
    image:
      registry: docker.io
      repository: curlimages/curl
      tag: 8.18.0@sha256:d94d07ba9e7d6de898b6d96c1a072f6f8266c687af78a74f380087a0addf5d17
  imageRenderer:
    image:
      registry: docker.io
      repository: grafana/grafana-image-renderer
      tag: v5.5.1@sha256:36e89efeb2a384e39ac06a3aaa5e5f44474dbd425c1353a150005fca31e4dcd7
  networkPolicy:
    type: none
    dnsLabels:
      io.kubernetes.pod.namespace: kube-system
      k8s-app: kube-dns
    metricsLabels:
      io.kubernetes.pod.namespace: monitoring
      app.kubernetes.io/name: prometheus
    ingressLabels:
      io.kubernetes.pod.namespace: '{{ eq .Values.ingress.provider "nginx" | ternary "ingress-nginx" "ingress" }}'
      app.kubernetes.io/name: '{{ eq .Values.ingress.provider "nginx" | ternary "ingress-nginx" "ingress-controller" }}'
  # it's important that the `url` is the first key and `charts` is right below, otherwise renovate won't detect it
  helmRepositories:
    prometheus:
      url: https://prometheus-community.github.io/helm-charts
      charts:
        kube-prometheus-stack: 81.5.2
      type: helm
      condition: "{{ .Values.monitoring.prometheus.enabled }}"
      interval: 5m
    grafana:
      url: https://grafana.github.io/helm-charts
      charts:
        loki: 6.53.0
        alloy: 1.5.3
      condition: "{{ .Values.monitoring.prometheus.enabled }}"
    grafana-community:
      url: https://grafana-community.github.io/helm-charts
      charts:
        tempo: 1.26.1
      condition: "{{ and .Values.monitoring.prometheus.enabled .Values.monitoring.tracing.enabled }}"
    external-dns:
      url: https://kubernetes-sigs.github.io/external-dns
      charts:
        external-dns: 1.20.0
      condition: '{{ not (empty .Values.dns.provider) }}'
    oauth2-proxy:
      url: https://oauth2-proxy.github.io/manifests
      charts:
        oauth2-proxy: 10.1.3
      condition: '{{ and (not (empty .Values.global.authentication.config)) .Values.monitoring.prometheus.enabled }}'
    metrics-server:
      url: https://kubernetes-sigs.github.io/metrics-server
      charts:
        metrics-server: 3.13.0
      condition: "{{ .Values.monitoring.metricsServer.enabled }}"
    descheduler:
      url: https://kubernetes-sigs.github.io/descheduler
      charts:
        descheduler: 0.34.0
        descheduler 0.31.x: 0.33.0
        descheduler 0.30.x: 0.32.2
        descheduler 0.29.x: 0.31.2
        descheduler 0.28.x: 0.30.2
        descheduler 0.27.x: 0.29.0
      condition: "{{ .Values.descheduler.enabled }}"
    jetstack:
      url: oci://quay.io/jetstack/charts
      charts:
        cert-manager: v1.17.2
    nginx:
      url: https://kubernetes.github.io/ingress-nginx
      charts:
        ingress-nginx: 4.12.3
      condition: '{{ eq .Values.ingress.provider "nginx" }}'
    traefik:
      url: https://helm.traefik.io/traefik
      charts:
        traefik: 39.0.0
      condition: '{{ eq .Values.ingress.provider "traefik" }}'
    kyverno:
      url: https://kyverno.github.io/kyverno
      charts:
        kyverno: 3.4.4
        kyverno-policies: 3.4.4
      condition: "{{ .Values.kyverno.enabled }}"
    tetragon:
      url: https://helm.cilium.io
      charts:
        tetragon: 1.6.0
      condition: "{{ .Values.tetragon.enabled }}"
    cetic:
      url: https://cetic.github.io/helm-charts
      charts:
        static: 0.1.1
    nfs-server-provisioner:
      url: https://kubernetes-sigs.github.io/nfs-ganesha-server-and-external-provisioner
      charts:
        nfs-server-provisioner: 1.8.0
      condition: "{{ .Values.storage.readWriteMany.enabled }}"
    teuto-net:
      url: oci://ghcr.io/teutonet/teutonet-helm-charts
    trivy:
      url: https://aquasecurity.github.io/helm-charts
      charts:
        trivy-operator: 0.31.0
      condition: "{{ .Values.monitoring.securityScanning.enabled }}"
    emberstack:
      url: https://emberstack.github.io/helm-charts
      charts:
        reflector: 10.0.8
      condition: '{{ include "base-cluster.reflector.enabled" (dict "context" .) }}'
    k8up:
      url: https://k8up-io.github.io/k8up
      charts:
        k8up: 4.8.6
      condition: '{{ ne (.Values.backup.provider).k8up nil }}'
    vmware:
      url: https://vmware-tanzu.github.io/helm-charts
      charts:
        velero: 7.2.2
      condition: '{{ ne (.Values.backup.provider).velero nil }}'
    kube-janitor:
      url: https://github.com/teutonet/kube-janitor
      charts:
        kube-janitor:
          path: unsupported/helm
          tag: 23.7.0
      type: git
      condition: '{{ dig "kube-janitor" "enabled" false .Values.AsMap }}'
  authentication:
    grafana:
      authenticationPath: /protocol/openid-connect/auth
      apiPath: /protocol/openid-connect/userinfo
      tokenPath: /protocol/openid-connect/token
      roleAttributePath: '"''Viewer''"'
    oauthProxy:
      emailDomains: []
      resourcesPreset: nano
      resources:
        limits:
          cpu: 100m
          memory: 32Mi
        requests:
          cpu: 10m
          memory: 16Mi
  autoConfiguration:
    cloudProvider: auto
    openstack:
      cloudConfiguration:
        type: Secret
        namespace: kube-system
        name: cloud-config
        field: cloud.conf

kyverno:
  enabled: false
  podSecurityStandard: baseline
  podSecuritySeverity: medium
  validationFailureAction: audit

monitoring:
  monitorAllNamespaces: true
  grafana:
    adminPassword: ""
    ingress:
      enabled: true
      host: grafana
      customDomain: ""
    additionalDashboards: {}
    additionalPlugins: []
    notifiers: []
    resourcesPreset: nano
    resources: {}
    persistence:
      enabled: false
      size: 10Gi
    config: {}
    sidecar:
      resourcesPreset: nano
      resources:
        limits:
          memory: 256Mi
  storageCostAnalysis:
    currency: currencyEUR
    period: Day
    storageClassMapping:
      teutostack-hdd: 0.002
      teutostack-ssd: 0.0067
  deadMansSwitch:
    enabled: false
    apiKey: ""
    pingKey: ""
  prometheus:
    enabled: true
    replicas: 2
    retentionDuration: 4w
    retentionSize: 90GB
    persistence:
      storageClass: ""
      size: 100Gi
    resourcesPreset: large
    resources:
      requests:
        cpu: "500m"
    operator:
      resourcesPreset: nano
      resources: {}
    kubeStateMetrics:
      resourcesPreset: nano
      resources: {}
      metricLabelsAllowList:
        pods:
          - app.kubernetes.io/name
          - app.kubernetes.io/component
          - app.kubernetes.io/instance
          - statefulset.kubernetes.io/pod-name
        deployments:
          - app.kubernetes.io/name
          - app.kubernetes.io/component
          - app.kubernetes.io/instance
        statefulsets:
          - app.kubernetes.io/name
          - app.kubernetes.io/component
          - app.kubernetes.io/instance
    nodeExporter:
      resourcesPreset: nano
      resources:
        requests:
          cpu: 10m
          memory: 32Mi
        limits:
          cpu: 2
          memory: 64Mi
    ingress:
      host: prometheus
      customDomain: ""
    alertmanager:
      defaultReceiver: '{{ hasKey .Values.monitoring.prometheus.alertmanager.receivers "pagerduty" | ternary "pagerduty" "null" }}'
      receivers: {}
      routes: []
      ingress:
        host: alertmanager
        customDomain: ""
      replicas: 3
      retentionDuration: 120h
      persistence:
        storageClass: ""
        size: 1Gi
  loki:
    enabled: true
    retentionPeriod: 45d
    persistence:
      storageClass: ""
      size: 10Gi
    resourcesPreset: nano
    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        cpu: 1
        memory: 1Gi
  alloy:
    resourcesPreset: nano
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 1
        memory: 128Mi
    gateway:
      resourcesPreset: small
  metricsServer:
    enabled: true
  securityScanning:
    enabled: true
    nodeCollector:
      tolerations:
        - key: node-role.kubernetes.io/control-plane
          operator: Exists
          effect: NoSchedule
  tracing:
    enabled: false
    resourcesPreset: small
    resources: {}
    persistence:
      size: 10Gi

descheduler:
  enabled: true
  resourcesPreset: nano
  resources:
    requests:
      cpu: 10m
      memory: 64Mi
    limits:
      cpu: 100m
      memory: 128Mi
  profile:
    pluginConfig:
      - name: DefaultEvictor
        args:
          evictLocalStoragePods: true
          nodeFit: true
      - name: RemoveDuplicates
      - name: RemovePodsViolatingNodeAffinity
        args:
          nodeAffinityType:
            - requiredDuringSchedulingIgnoredDuringExecution
      - name: RemovePodsViolatingNodeTaints
      - name: RemovePodsViolatingInterPodAntiAffinity
      - name: RemovePodsViolatingTopologySpreadConstraint
        args:
          topologyBalanceNodeFit: true
      - name: LowNodeUtilization
        args:
          thresholds:
            cpu: 50
            memory: 50
            pods: 50
          targetThresholds:
            cpu: 70
            memory: 80
            pods: 95
    plugins:
      balance:
        enabled:
          - RemoveDuplicates
          - RemovePodsViolatingTopologySpreadConstraint
          - LowNodeUtilization
      deschedule:
        enabled:
          - RemovePodsHavingTooManyRestarts
          - RemovePodsViolatingNodeTaints
          - RemovePodsViolatingNodeAffinity
          - RemovePodsViolatingInterPodAntiAffinity

dns:
  domains: []
  provider: null

certManager:
  resourcesPreset: nano
  resources: {}
  caInjector:
    resourcesPreset: nano
    resources: {}
  webhook:
    resourcesPreset: nano
    resources: {}
  dnsChallengeNameservers:
    1.1.1.1: 53

externalDNS:
  resourcesPreset: nano
  resources:
    requests:
      cpu: 10m
      memory: 64Mi
    limits:
      cpu: 50m
      memory: 128Mi

flux:
  gitRepositories: {}

ingress:
  provider: traefik
  allowNginxConfigurationSnippets: false
  useProxyProtocol: auto
  replicas: 2
  resourcesPreset: nano
  resources:
    limits:
      cpu: 1

storage:
  readWriteMany:
    enabled: false
    storageClass:
      name: teutostack-nfs
    persistence:
      storageClass: ""
      size: 5Gi

reflector:
  enabled: auto

rbac:
  roles: {}
  accounts: {}

backup:
  provider: null
  resources:
    requests:
      cpu: 5m
      memory: 25Mi
    limits:
      cpu: 500m
      memory: 500Mi
  runners:
    resources:
      requests:
        cpu: 5m
        memory: 25Mi
      limits:
        cpu: 500m
        memory: 500Mi

kube-janitor:
  enabled: false

tetragon:
  enabled: false
