{{- define "base-cluster.prometheus.config" -}}

  {{- if .Values.global.imageRegistry }}
global:
  imageRegistry: {{ .Values.global.imageRegistry }}
  {{- end }}
grafana:
  {{- if $.Values.global.imageRegistry }}
  image:
    repository: {{ printf "%s/grafana/grafana" .Values.global.imageRegistry }}
  downloadDashboardsImage:
    repository: {{ printf "%s/curlimages/curl" .Values.global.imageRegistry }}
  {{- end }}
  imageRenderer:
    enabled: true
    securityContext:
      seccompProfile:
        type: RuntimeDefault
      runAsNonRoot: true
      runAsUser: 472
      runAsGroup: 472
  {{- if $.Values.global.imageRegistry }}
    image:
      repository: {{ printf "%s/grafana/grafana-image-renderer" .Values.global.imageRegistry }}
  {{- end }}
  enabled: true
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  containerSecurityContext: &containerSecurityContext
    capabilities:
      drop:
        - ALL
    allowPrivilegeEscalation: false
    privileged: false
  resources: {{- .Values.monitoring.grafana.resources | toYaml | nindent 4 }}
  serviceMonitor:
    interval: "30s"
    labels: {{- .Values.monitoring.labels | toYaml | nindent 6 }}
  podAnnotations:
    # This might change on every `template` call, this can be ignored
    checksum/secret: {{ include (print $.Template.BasePath "/monitoring/kube-prometheus-stack/grafana-secret.yaml") . | sha256sum }}
  admin:
    existingSecret: {{ include "common.secrets.name" (dict "defaultNameSuffix" "grafana" "context" $) }}
    userKey: username
    passwordKey: password
  plugins:
    - grafana-piechart-panel
  {{- with .Values.monitoring.grafana.additionalPlugins }}
  {{- . | toYaml | nindent 4 }}
  {{- end }}
  defaultDashboardsEnabled: true
  grafana.ini: {{- .Values.monitoring.grafana.config | mergeOverwrite (include "base-cluster.grafana.config" . | fromYaml) | toYaml | nindent 4 -}}
  {{- if .Values.monitoring.grafana.notifiers }}
  extraEmptyDirMounts:
    - name: provisioning-notifiers
      mountPath: /etc/grafana/provisioning/notifiers
  notifiers:
    notifiers.yaml:
      notifiers: {{- .Values.monitoring.grafana.notifiers | toYaml | nindent 8 }}
  {{- end }}
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: 'custom'
          orgId: 1
          folder: ''
          type: file
          disableDeletion: true
          editable: true
          options:
            path: /var/lib/grafana/dashboards/custom
  sidecar:
    {{- if .Values.global.imageRegistry }}
    image:
      repository: {{ printf "%s/kiwigrid/k8s-sidecar" .Values.global.imageRegistry }}
    {{- end }}
    dashboards:
      enabled: true
      resource: configmap
      searchNamespace: ALL
    datasources:
      enabled: true
      resource: configmap
      searchNamespace: ALL
    resources: {{- .Values.monitoring.grafana.sidecar.resources | toYaml | nindent 6 }}
    securityContext: *containerSecurityContext
  dashboards:
    custom:
      #capacity-planning:
      #  gnetId: 22
      #  revision: 7
      #  datasource: Prometheus
      kubernetes-cluster-monitoring-via-prometheus: &dashboard
        datasource: Prometheus
        gnetId: 315
        revision: 3
      node-exporter-full:
        <<: *dashboard
        gnetId: 1860
        revision: 16
      metrics:
        <<: *dashboard
        gnetId: 8588
      ingress-nginx:
        <<: *dashboard
        gnetId: 9614
        revision: 1
      cert-manager:
        <<: *dashboard
        gnetId: 11001
        revision: 1
      {{- if .Values.dns.provider }}
      external-dns:
        <<: *dashboard
        gnetId: 15038
        revision: 1
      {{- end }}
      {{- if and .Values.monitoring.loki.enabled false }}
      # TODO: this one is broken
      loki:
        <<: *dashboard
        gnetId: 12611
        revision: 1
        datasource: Loki
      {{- end }}
      {{- if .Values.kyverno.enabled }}
      kyverno:
        <<: *dashboard
        gnetId: 15804
        revision: 4
      {{- end }}
  {{- with .Values.monitoring.grafana.additionalDashboards }}
  {{ . | toYaml | nindent 6 }}
  {{- end }}
  {{- if and .Values.certManager.email .Values.monitoring.grafana.ingress.enabled .Values.global.baseDomain }}
  ingress:
    enabled: true
    ingressClassName: nginx
    annotations:
      kubernetes.io/tls-acme: "true"
    hosts:
      - {{ include "base-cluster.grafana.host" $ }}
    tls:
      - hosts:
          - {{ include "base-cluster.grafana.host" $ }}
        secretName: {{ include "base-cluster.certificate" (dict "name" "grafana" "context" $) }}
  {{- end }}
  downloadDashboards:
    securityContext: *containerSecurityContext
  initChownData:
    enabled: false
prometheusOperator:
  secretFieldSelector: 'type!=helm.sh/release.v1'
  resources: {{- .Values.monitoring.prometheus.operator.resources | toYaml | nindent 6 }}
  priorityClassName: system-cluster-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  containerSecurityContext:
    capabilities:
      drop:
        - ALL
defaultRules:
  disabled:
    etcdHighNumberOfFailedGRPCRequests: true
  rules:
    kubeApiserverAvailability: false
kubelet:
  serviceMonitor:
    resource: false
kubeEtcd:
  service:
    targetPort: 2379
  serviceMonitor:
    scheme: https
    caFile: /etc/prometheus/secrets/etcd-certs/ca.crt
    certFile: /etc/prometheus/secrets/etcd-certs/client.crt
    keyFile: /etc/prometheus/secrets/etcd-certs/client.key
commonLabels: {{- .Values.monitoring.labels | toYaml | nindent 2 }}
kube-state-metrics:
  {{- if .Values.global.imageRegistry }}
  image:
    repository: {{ printf "%s/kube-state-metrics/kube-state-metrics" .Values.global.imageRegistry }}
  {{- end }}
  resources: {{- .Values.monitoring.prometheus.kubeStateMetrics.resources | toYaml | nindent 6 }}
  {{- $metricLabelsAllowList := list -}}
  {{- range $resource, $labels := .Values.monitoring.prometheus.kubeStateMetrics.metricLabelsAllowList -}}
  {{- $metricLabelsAllowList = append $metricLabelsAllowList (printf "%s=[%s]" $resource ($labels | join ",") ) -}}
  {{- end }}
  extraArgs:
    - --metric-labels-allowlist={{- $metricLabelsAllowList | join "," }}
  priorityClassName: system-cluster-critical
  securityContext:
    enabled: true
    seccompProfile:
      type: RuntimeDefault
    runAsNonRoot: true
  containerSecurityContext: *containerSecurityContext
  prometheus:
    monitor:
      additionalLabels: {{- .Values.monitoring.labels | toYaml | nindent 8 }}
prometheus-node-exporter:
  {{- if .Values.global.imageRegistry }}
  image:
    repository: {{ printf "%s/prometheus/node-exporter" .Values.global.imageRegistry }}
  {{- end }}
  resources: {{- .Values.monitoring.prometheus.nodeExporter.resources | toYaml | nindent 6 }}
  priorityClassName: system-cluster-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  containerSecurityContext: *containerSecurityContext
  prometheus:
    monitor:
      additionalLabels: {{- .Values.monitoring.labels | toYaml | nindent 8 }}
alertmanager:
  enabled: false # TODO dependent on routes/receivers
  # TODO routes
  # TODO receivers
  {{- if false }}
  podDisruptionBudget:
    enabled: true
  {{- if false }} # TODO ingress
{{/*  {{- if not (empty .Values.monitoring.prometheus.authentication.enabled | ternary .Values.global.authentication.enabled .Values.monitoring.prometheus.authentication.enabled) }}*/}}
  ingress:
    enabled: true
    hosts:
      - {{ include "base-cluster.alertmanager.host" $ }}
    tls:
      - hosts:
          - {{ include "base-cluster.alertmanager.host" $ }}
        secretName: cluster-wildcard-certificate
  {{- end }}
  alertmanagerSpec:
    replicas: {{ .Values.monitoring.prometheus.alertmanager.replicas }}
    podAntiAffinity: soft
    {{- if false }} # TODO ingress
{{/*    {{- if empty .Values.monitoring.prometheus.authentication.enabled | ternary .Values.global.authentication.enabled .Values.monitoring.prometheus.authentication.enabled }}*/}}
    externalUrl: https://{{ include "base-cluster.alertmanager.host" $ }}
    {{- end }}
    retention: {{ .Values.monitoring.prometheus.alertmanager.retentionDuration }}
    priorityClassName: system-cluster-critical
    storageSpec:
      volumeClaimTemplate:
        spec: {{- include "common.storage.class" ( dict "persistence" .Values.monitoring.prometheus.alertmanager.persistence "global" $) | nindent 10 }}
          accessModes:
            - "ReadWriteOnce"
          resources:
            requests:
              storage: {{ .Values.monitoring.prometheus.alertmanager.persistence.size }}
    alertmanagerConfigSelector:
      matchLabels: {{- .Values.monitoring.labels | toYaml | nindent 8 }}
  {{- end }}
prometheus:
  enabled: true
  {{- if false }} # TODO ingress
{{/*  {{- if not (empty .Values.monitoring.prometheus.authentication.enabled | ternary .Values.global.authentication.enabled .Values.monitoring.prometheus.authentication.enabled) }}*/}}
  ingress:
    enabled: true
    hosts:
      - {{ include "base-cluster.prometheus.host" $ }}
    tls:
      - hosts:
          - {{ include "base-cluster.prometheus.host" $ }}
        secretName: cluster-wildcard-certificate
  {{- end }}
  prometheusSpec:
    {{- if (lookup "v1" "Secret" "monitoring" "additional-alertmanager-configs") }}
    additionalAlertManagerConfigsSecret:
      name: additional-alertmanager-configs
      key: prometheus-additional-alertmanager-configs.yaml
    {{- end }}
    {{- if false }} # TODO ingress
{{/*    {{- if empty .Values.monitoring.prometheus.authentication.enabled | ternary .Values.global.authentication.enabled .Values.monitoring.prometheus.authentication.enabled }}*/}}
    externalUrl: {{ printf "https://%s" (include "base-cluster.prometheus.host" $) }}
    {{- end }}
    resources: {{- .Values.monitoring.prometheus.resources | toYaml | nindent 6 }}
    priorityClassName: system-cluster-critical
    securityContext:
      seccompProfile:
        type: RuntimeDefault
    serviceMonitorSelectorNilUsesHelmValues: false
    ruleSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    probeSelectorNilUsesHelmValues: false
    retention: {{ .Values.monitoring.prometheus.retentionDuration }}
    retentionSize: {{ .Values.monitoring.prometheus.retentionSize }}
    storageSpec:
      volumeClaimTemplate:
        spec: {{- include "common.storage.class" ( dict "persistence" .Values.monitoring.prometheus.persistence "global" $) | nindent 10 }}
          accessModes:
            - "ReadWriteOnce"
          resources:
            requests:
              storage: {{ .Values.monitoring.prometheus.persistence.size }}
    replicas: {{ .Values.monitoring.prometheus.replicas }}
    ruleSelector:
      matchLabels: {{- .Values.monitoring.labels | toYaml | nindent 8 }}
    serviceMonitorSelector:
      matchLabels: {{- .Values.monitoring.labels | toYaml | nindent 8 }}
    podMonitorSelector:
      matchLabels: {{- .Values.monitoring.labels | toYaml | nindent 8 }}
    probeSelector:
      matchLabels: {{- .Values.monitoring.labels | toYaml | nindent 8 }}
    additionalAlertRelabelConfigs:
      - # TODO: Needs alternative
        source_labels:
          - namespace
        action: keep
        regex: (cert-manager|flux-system|ingress-nginx|kube-node-lease|kube-public|kube-system|kyverno|monitoring)
      - source_labels:
          - __address__
        target_label: clustertype
        replacement: kubernetes # TODO: really needed?

      - source_labels:
          - __address__
        target_label: teutosla
        replacement: {{ .Values.global.serviceLevelAgreement }}
      - source_labels:
          - __address__
        target_label: cluster
        replacement: {{ .Values.global.clusterName }}
    secrets:
      - etcd-certs
{{- end -}}

{{- define "base-cluster.grafana.config" -}}
{{- if and .Values.certManager.email .Values.global.baseDomain -}}
auth:
  signout_redirect_url: {{ printf "https://%s" (include "base-cluster.grafana.host" .) }}
server:
  root_url: {{ printf "https://%s" (include "base-cluster.grafana.host" .) }}
{{- end -}}
{{- end -}}
